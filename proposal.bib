
@article{hindman_mesos_nodate,
	title = {Mesos: {A} {Platform} for {Fine}-{Grained} {Resource} {Sharing} in the {Data} {Center}},
	abstract = {We present Mesos, a platform for sharing commodity clusters between multiple diverse cluster computing frameworks, such as Hadoop and MPI. Sharing improves cluster utilization and avoids per-framework data replication. Mesos shares resources in a ﬁne-grained manner, allowing frameworks to achieve data locality by taking turns reading data stored on each machine. To support the sophisticated schedulers of today’s frameworks, Mesos introduces a distributed two-level scheduling mechanism called resource offers. Mesos decides how many resources to offer each framework, while frameworks decide which resources to accept and which computations to run on them. Our results show that Mesos can achieve near-optimal data locality when sharing the cluster among diverse frameworks, can scale to 50,000 (emulated) nodes, and is resilient to failures.},
	language = {en},
	author = {Hindman, Benjamin and Konwinski, Andy and Zaharia, Matei and Ghodsi, Ali and Joseph, Anthony D and Katz, Randy and Shenker, Scott and Stoica, Ion},
	file = {Hindman et al. - Mesos A Platform for Fine-Grained Resource Sharin.pdf:/Users/hamza/Zotero/storage/I8RVS2JD/Hindman et al. - Mesos A Platform for Fine-Grained Resource Sharin.pdf:application/pdf},
}

@misc{noauthor_mercedes-benz_2023,
	title = {How {Mercedes-Benz} Expanded Its Kubernetes Fleet Management with Cluster {API} to Public Clouds},
	author = {Tobias Giese, Sean Schneeweiss},
	url = {https://www.cncf.io/case-studies/mercedes-benz/},
	howpublished = {\url{https://www.cncf.io/case-studies/mercedes-benz/}},
    abstract = {Mercedes-Benz is one of the most successful automotive companies in the world. As a 100\% subsidiary of Mercedes-Benz, Mercedes-Benz Tech Innovation GmbH creates digital products and software solutions…},
	language = {en-US},
	urldate = {2023-07-31},
	journal = {Cloud Native Computing Foundation},
	month = jun,
	year = {2023},
}

@misc{airbnb,
	title = {Dynamic {Kubernetes} {Cluster} {Scaling} at {Airbnb} {\textbar} by {David} {Morrison} {\textbar} {The} {Airbnb} {Tech} {Blog} {\textbar} {Medium}},
	url = {https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132},
    howpublished = {\url{https://medium.com/airbnb-engineering/dynamic-kubernetes-cluster-scaling-at-airbnb-d79ae3afa132}},
	urldate = {2023-07-31},
}

@inproceedings{zaharia_delay_2010,
	address = {Paris France},
	title = {Delay scheduling: a simple technique for achieving locality and fairness in cluster scheduling},
	isbn = {978-1-60558-577-2},
	shorttitle = {Delay scheduling},
	url = {https://dl.acm.org/doi/10.1145/1755913.1755940},
	doi = {10.1145/1755913.1755940},
	abstract = {As organizations start to use data-intensive cluster computing systems like Hadoop and Dryad for more applications, there is a growing need to share clusters between users. However, there is a conﬂict between fairness in scheduling and data locality (placing tasks on nodes that contain their input data). We illustrate this problem through our experience designing a fair scheduler for a 600-node Hadoop cluster at Facebook. To address the conﬂict between locality and fairness, we propose a simple algorithm called delay scheduling: when the job that should be scheduled next according to fairness cannot launch a local task, it waits for a small amount of time, letting other jobs launch tasks instead. We ﬁnd that delay scheduling achieves nearly optimal data locality in a variety of workloads and can increase throughput by up to 2x while preserving fairness. In addition, the simplicity of delay scheduling makes it applicable under a wide variety of scheduling policies beyond fair sharing.},
	language = {en},
	urldate = {2023-08-02},
	booktitle = {Proceedings of the 5th {European} conference on {Computer} systems},
	publisher = {ACM},
	author = {Zaharia, Matei and Borthakur, Dhruba and Sen Sarma, Joydeep and Elmeleegy, Khaled and Shenker, Scott and Stoica, Ion},
	month = apr,
	year = {2010},
	pages = {265--278},
	file = {Zaharia et al. - 2010 - Delay scheduling a simple technique for achieving.pdf:/Users/hamza/Zotero/storage/STFXM2WK/Zaharia et al. - 2010 - Delay scheduling a simple technique for achieving.pdf:application/pdf},
}

@inproceedings{patel_what_2022,
	title = {What does {Inter}-{Cluster} {Job} {Submission} and {Execution} {Behavior} {Reveal} to {Us}?},
	doi = {10.1109/CLUSTER51413.2022.00019},
	abstract = {Modern High Performing Computing (HPC) facil-ities have multiple computing clusters that serve different pur-poses. These include large-scale computing clusters and smaller data visualization and analysis clusters, which are meant to shift the load of data analytics jobs from the large-scale systems. We perform the first in-depth characterization of cross-cluster behavior of users and jobs and provide an analysis of three inter-related systems at the Argonne Leadership Computing Facility (ALCF). Our analysis reveals interesting trends related to the resource utilization and predictability of user and job behavior across different clusters.},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Patel, Tirthak and Tiwari, Devesh and Kettimuthu, Raj and Allcock, William and Rich, Paul and Liu, Zhengchun},
	month = sep,
	year = {2022},
	note = {ISSN: 2168-9253},
	keywords = {Cross-Cluster Analysis, Data analysis, Data centers, Data models, Data visualization, High-Performance Computing, Large-Scale Systems, Leadership, Market research, Predictive models, Supercomputing},
	pages = {35--46},
	file = {IEEE Xplore Abstract Record:/Users/hamza/Zotero/storage/NMN8LBMR/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/hamza/Zotero/storage/YCYSEJ9B/Patel et al. - 2022 - What does Inter-Cluster Job Submission and Executi.pdf:application/pdf},
}

@inproceedings{lo_heracles_2015,
	address = {Portland Oregon},
	title = {Heracles: improving resource efficiency at scale},
	isbn = {978-1-4503-3402-0},
	shorttitle = {Heracles},
	url = {https://dl.acm.org/doi/10.1145/2749469.2749475},
	doi = {10.1145/2749469.2749475},
	abstract = {User-facing, latency-sensitive services, such as websearch, underutilize their computing resources during daily periods of low trafﬁc. Reusing those resources for other tasks is rarely done in production services since the contention for shared resources can cause latency spikes that violate the service-level objectives of latency-sensitive tasks. The resulting under-utilization hurts both the affordability and energy-efﬁciency of large-scale datacenters. With technology scaling slowing down, it becomes important to address this opportunity.},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Proceedings of the 42nd {Annual} {International} {Symposium} on {Computer} {Architecture}},
	publisher = {ACM},
	author = {Lo, David and Cheng, Liqun and Govindaraju, Rama and Ranganathan, Parthasarathy and Kozyrakis, Christos},
	month = jun,
	year = {2015},
	pages = {450--462},
	file = {Lo et al. - 2015 - Heracles improving resource efficiency at scale.pdf:/Users/hamza/Zotero/storage/4MPWHPR7/Lo et al. - 2015 - Heracles improving resource efficiency at scale.pdf:application/pdf},
}

@misc{wilkes_yet_2020,
	title = {Yet more {Google} compute cluster trace data},
	author = {Wilkes, John},
	month = apr,
	year = {2020},
	note = {Place: Mountain View, CA, USA
Published: Google research blog},
}

@inproceedings{bhattacharya_hierarchical_2013,
	address = {Santa Clara California},
	title = {Hierarchical scheduling for diverse datacenter workloads},
	isbn = {978-1-4503-2428-1},
	url = {https://dl.acm.org/doi/10.1145/2523616.2523637},
	doi = {10.1145/2523616.2523637},
	abstract = {There has been a recent industrial effort to develop multi-resource hierarchical schedulers. However, the existing implementations have some shortcomings in that they might leave resources unallocated or starve certain jobs. This is because the multi-resource setting introduces new challenges for hierarchical scheduling policies. We provide an algorithm, which we implement in Hadoop, that generalizes the most commonly used multi-resource scheduler, DRF [1], to support hierarchies. Our evaluation shows that our proposed algorithm, H-DRF, avoids the starvation and resource inefﬁciencies of the existing open-source schedulers and outperforms slot scheduling.},
	language = {en},
	urldate = {2023-01-19},
	booktitle = {Proceedings of the 4th annual {Symposium} on {Cloud} {Computing}},
	publisher = {ACM},
	author = {Bhattacharya, Arka A. and Culler, David and Friedman, Eric and Ghodsi, Ali and Shenker, Scott and Stoica, Ion},
	month = oct,
	year = {2013},
	pages = {1--15},
	file = {Bhattacharya et al. - 2013 - Hierarchical scheduling for diverse datacenter wor.pdf:/Users/hamza/Zotero/storage/L6M67DXV/Bhattacharya et al. - 2013 - Hierarchical scheduling for diverse datacenter wor.pdf:application/pdf},
}

@inproceedings{li_lyra_2023,
	address = {Rome Italy},
	title = {Lyra: {Elastic} {Scheduling} for {Deep} {Learning} {Clusters}},
	isbn = {978-1-4503-9487-1},
	shorttitle = {Lyra},
	url = {https://dl.acm.org/doi/10.1145/3552326.3587445},
	doi = {10.1145/3552326.3587445},
	language = {en},
	urldate = {2023-09-19},
	booktitle = {Proceedings of the {Eighteenth} {European} {Conference} on {Computer} {Systems}},
	publisher = {ACM},
	author = {Li, Jiamin and Xu, Hong and Zhu, Yibo and Liu, Zherui and Guo, Chuanxiong and Wang, Cong},
	month = may,
	year = {2023},
	pages = {835--850},
	file = {Full Text PDF:/Users/hamza/Zotero/storage/5JU3FQPZ/Li et al. - 2023 - Lyra Elastic Scheduling for Deep Learning Cluster.pdf:application/pdf},
}

@misc{noauthor_cncf_2023,
	title = {{CNCF} {Annual} {Survey} 2022},
	author = {Cloud native computing foundation, The Linux foundation}
	howpublished = {\url{https://www.cncf.io/reports/cncf-annual-survey-2022/}},
	language = {en-US},
	urldate = {2023-09-19},
	journal = {Cloud Native Computing Foundation},
	month = jan,
	year = {2023},
	file = {Snapshot:/Users/hamza/Zotero/storage/W6MRZP49/cncf-annual-survey-2022.html:text/html},
}

@misc{sched-github,
    title = {Project Simulator},
    url = {https://github.com/hamzalsheikh/scheduling-sandbox},
    howpublished = {\url{https://github.com/hamzalsheikh/scheduling-sandbox}}
}


@misc{google-cloud-blog,
	journal = {Google Cloud},
	language = {en},
	title = {Multi-cluster use cases {\textbar} {Google} {Fleet} management},
	url = {https://cloud.google.com/anthos/fleet-management/docs/multi-cluster-use-cases},
	howpublished = {\url{https://cloud.google.com/anthos/fleet-management/docs/multi-cluster-use-cases}},
	urldate = {2024-01-02},
	bdsk-url-1 = {https://cloud.google.com/anthos/fleet-management/docs/multi-cluster-use-cases}}


@misc{adv-dis-mutli,
	author = "Morgan Perry",
	abstract = {Over the years, Kubernetes has stood out as one of the best platforms for container orchestration. However, managing Kubernetes is still complex. The first question which comes to mind is whether we should use a single cluster or a multi-cluster for Kubernetes. In many cases, a single cluster is not enough to manage the load efficiently across all components. As a result, we need more than one cluster for a better division of workload and resources, hence the need for a multi-cluster solution. In this article, we will discuss in detail multi-cluster Kubernetes, why it is used, and when we should prefer it over single-cluster.},
	file = {Snapshot:/Users/hamza/Zotero/storage/M7YSLLAX/kubernetes-multi-cluster-why-and-when-to-use-them.html:text/html},
	language = {en},
	shorttitle = {Kubernetes {Multi}-{Cluster}},
	title = {Kubernetes {Multi}-{Cluster}: {Why} and {When} {To} {Use} {Them}},
	url = {https://www.qovery.com/blog/kubernetes-multi-cluster-why-and-when-to-use-them/},
	urldate = {2024-01-02},
	bdsk-url-1 = {https://www.qovery.com/blog/kubernetes-multi-cluster-why-and-when-to-use-them/}}


@misc{opentelemetry,
	abstract = {High-quality, ubiquitous, and portable telemetry to enable effective observability},
	file = {Snapshot:/Users/hamza/Zotero/storage/HSEKPIZZ/opentelemetry.io.html:text/html},
	journal = {OpenTelemetry},
	language = {en},
	title = {{OpenTelemetry}},
	url = {https://opentelemetry.io/},
	howpublished = {\url{https://opentelemetry.io/}}
	urldate = {2024-01-02},
	bdsk-url-1 = {https://opentelemetry.io/}}


@misc{jaeger,
	abstract = {Monitor and troubleshoot workflows in complex distributed systems},
	file = {Snapshot:/Users/hamza/Zotero/storage/9BB93L5B/www.jaegertracing.io.html:text/html},
	language = {en},
	shorttitle = {Jaeger},
	title = {Jaeger: open source, distributed tracing platform},
	url = {https://www.jaegertracing.io/},
	howpublished = {\url{https://www.jaegertracing.io/}}
	urldate = {2024-01-02},
	bdsk-url-1 = {https://www.jaegertracing.io/}}


@misc{google-cluster-data2019,
	file = {Snapshot:/Users/hamza/Zotero/storage/7U7NWV8L/ClusterData2019.html:text/html},
	language = {en},
	title = {cluster-data/{ClusterData2019}.md at master · google/cluster-data},
	url = {https://github.com/google/cluster-data/blob/master/ClusterData2019.md},
	urldate = {2024-01-02},
	bdsk-url-1 = {https://github.com/google/cluster-data/blob/master/ClusterData2019.md}}


@misc{alibabaclusterdata_2023,
	title = {cluster data collected from production clusters in Alibaba for cluster management research},
	keywords = {dataset},
	month = dec,
	year = 2023,
	publisher = {Alibaba},
	title = {alibaba/clusterdata},
	url = {https://github.com/alibaba/clusterdata},
	howpublished = {\url{https://github.com/alibaba/clusterdata}}
	urldate = {2024-01-02},
	year = {2023},
	bdsk-url-1 = {https://github.com/alibaba/clusterdata}}

@inproceedings{borg,
title	= {Large-scale cluster management at {Google} with {Borg}},
author	= {Abhishek Verma and Luis Pedrosa and Madhukar R. Korupolu and David Oppenheimer and Eric Tune and John Wilkes},
year	= {2015},
booktitle	= {Proceedings of the European Conference on Computer Systems (EuroSys)},
address	= {Bordeaux, France}}

@inproceedings {alibaba-latest,
author = {Qizhen Weng and Lingyun Yang and Yinghao Yu and Wei Wang and Xiaochuan Tang and Guodong Yang and Liping Zhang},
title = {Beware of Fragmentation: Scheduling {GPU-Sharing} Workloads with Fragmentation Gradient Descent},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {995--1008},
url = {https://www.usenix.org/conference/atc23/presentation/weng},
publisher = {USENIX Association},
month = jul
}